services:
  llama:
    build: .
    container_name: llama-server
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    group_add:
      - video
    shm_size: "16g"
    ports:
      - "8080:8080"
    volumes:
      - ./model:/model
    environment:
      - HSA_OVERRIDE_GFX_VERSION=9.0.6
      - HSA_XNACK=0
      - HIP_VISIBLE_DEVICES=0,1
    command:
      - --model
      - /model/Qwen3-Next-80B-A3B-Instruct-Q4_K_S.gguf
      - --port
      - "8080"
      - --host
      - "0.0.0.0"
      - --n-gpu-layers
      - "999"
      - --split-mode
      - layer
      - --no-mmap
    restart: unless-stopped
